<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ObjectRelator: Enabling Cross-View Object Relation Understanding Across Ego-Centric and Exo-Centric Perspectives">
  <meta name="keywords" content="Cross-View Object Relation Understanding · Egocentric Vision · Ego-Exo4D">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ObjectRelator</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://lovelyqian.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"> ObjectRelator: Enabling Cross-View Object Relation Understanding Across Ego-Centric and Exo-Centric Perspectives</h1>
          <h2><font color=#2980B9 size="5">ICCV 2025</font></h2> <br>
          <div class="is-size-5 publication-authors">
               <span class="author-block">
              <div>Yuqian Fu<sup>1</sup>, </div>
            </span>
            <span class="author-block">
              <div>Runze Wang<sup>2</sup>, </div>
            </span>
            <span class="author-block">
              <div>Bin Ren<sup>1,3,4</sup>, </div>
            </span>
            <span class="author-block">
              <div>Guolei Sun<sup>5</sup>, </div>
            </span>
            <span class="author-block">
              <div>Biao Gong<sup>6</sup>, </div>  
            </span> <br>
            <span class="author-block">
              <div>Yanwei Fu<sup>2</sup>, </div>
            </span>
            <span class="author-block">
              <div>Danda Pani Paudel<sup>1</sup>, </div>
            </span>
            <span class="author-block">
              <div>Xuanjing Huang<sup>2</sup>, </div>
            </span>
            <span class="author-block">
              <div>Luc Van Gool<sup>1</sup></div>
            </span>
          </div>

          <h2><font color=#424949 size="3"> <sup>1</sup>INSAIT, <sup>2</sup>Fudan University, <sup>3</sup>University of Trento, <sup>4</sup>University of Pisa, <sup>5</sup>ETH Zurich, <sup>6</sup>Ant Group</font></h2>
</font></h2>
          <br>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2411.19083"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/lovelyqian/ObjectRelator"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Data & Code</span>
                  </a>
              </span>
              <!-- Video Link. -->
             <span class="link-block">
               <a href=""
                  class="external-link button is-normal is-rounded is-dark">
                 <span class="icon">
                     <i class="fab fa-youtube"></i>
                 </span>
                 <span>Video (coming soon) </span>
               </a>
             </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <center>
            <img src="./imgs/benchmark.png" width="95%"/>
            </center>
            <p>
            <b>CD-FSOD benchmark:</b> <font color=#2980B9>COCO</font> serves as the training source data, while six datasets including <font color=#2980B9>ArTaxOr</font>, <font color=#2980B9>Clipart1k</font>, <font color=#2980B9>DIOR</font>, <font color=#2980B9>DeepFish</font>, <font color=#2980B9>NEU-DET</font>, and <font color=#2980B9>UODD</font> are utilized as novel testing target datasets.
            <b> Dataset metrics:</b> We introduce <font color=#2980B9>styles</font>, inter-class variance (<font color=#2980B9>ICV</font>), and indefinable boundaries (<font color=#2980B9>IB</font>) as metrics for evaluating the domain gap issue in CD-FSOD. Our target datasets exhibit variations in these metrics.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivations</h2>
        <div class="content has-text-justified">
          <p>
          TODO Cross-domain Few-Shot Learning (CD-FSL) aims at transferring knowledge from a source dataset to new target domains with few labeled data. 
          However, most of the exitsing CD-FSL works focus on the classification task, overlooking <font color=#2980B9><b>object detection</b></font>. Thus, this paper delves into object detection tasks in CD-FSL, also known as <font color=#2980B9><b>cross-domain few-shot object detection (CD-FSOD)</b></font>.
          Previous traditional FSOD methods could roughtly grouped into meta-learning based ones and finetuning based ones, while a recent transformer-based open-set detector, DE-ViT, shows exceptional performance in FSOD, surpassing other methods as depicted in the below figure. This inspired us to study:
          <ol>
            <li> <em></em>Can such <b><font color=#2980B9> open-set detection methods easily generalize</font></b> to CD-FSOD?</em>  </li>
            <li> <em></em>If not, how can <b><font color=#2980B9> models be enhanced</font></b> when facing huge domain gaps?</em>  </li>
          </pl>
        </p>
          <center>
            <img src="./imgs/motivation.png" width="95%"/>
          </center>
          <p>
            <b>a) Our motivation:</b> The DE-ViT open-set detector excels in FSOD but strug-gles in CD-FSOD, inspiring our creation of CD-ViTO. <b>(b) Technical motivation:</b> FSOD
models face challenges when dealing with cross-domain targets, such as small inter-class variance (ICV), indefinable boundaries (IB), and varying appearances (styles).
          </p>
          <br>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Contributions</h2>
        <div class="content has-text-justified">
          <p>   
          <p>
            <b><h3> >> To answer the first question:</h3></b></b>
            <ul> 
              <li> We employ <font color=#2980B9><b>metrics</b></font> including style, ICV, and IB to understand the domain gap in CD-FSOD. 
              </li>
              <li> Based on these metrics, we establish <font color=#2980B9><b>a new benchmark</b></font> with diverse targets for CD-FSOD. 
              </li>
              <li> We conduct <font color=#2980B9><b>extensive study</b></font> of existing detectors revealing the challenges posed by CD-FSOD. 
              </li>
            </ul>
          </p>
          <p>
            <b><h3> >> To answer the second question:</h3></b></b>
            We build a new <b> <font color=#2980B9>CD-ViTO</font></b> method via enhancing the existing DE-ViT with the following novel modules:
            <ul> 
              <li> <b><font color=#2980B9>Learnable Instance Features:</font></b> align initial fixed instances with target categories, thus tackle the <b>small ICV</b> issue by enhancing feature distinctiveness. 
              </li>
              <li> <b><font color=#2980B9>Instance Reweighting Module:</font></b> assigns higher importance to high-quality instances with slight IB, thus alleviate the <b>significant IB</b> issue.
              </li>
              <li> <b><font color=#2980B9>Domain Prompter:</font></b> encourages features resilient to different styles by synthesizing imaginary domains without altering semantic contents, thus improve model's robustness to <b>different styles</b>. 
              </li>
            </ul>
          As indicated in the above figure, our <b><font color=#2980B9>enhanced CD-ViTO (orange stars) makes the DE-ViT (green stars) great again</font></b> on CD-FSOD targets.
          </p>


        </div>
      </div>
    </div>
  </div>
 
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Datasets</h2>
        <div class="content has-text-justified">
          <p>
            In addition to the visual examples as shown in the benchmark figure, we further provide more infomations here. <b>All the target datasets could be found on our <a href="https://github.com/lovelyqian/CDFSOD-benchmark">github repo</a>.</b>
          </p>
          <center>
            <img src="./imgs/datainfo.png" width="95%"/>
          </center>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">CD-ViTO Method</h2>
        <div class="content has-text-justified">
          <b>Overall framework</b> of our CD-ViTO: 

          <center>
            <img src="./imgs/framework.png" width="95%"/>
          </center>

          <p>
             We build our method upon the base open-set detector (DE-ViT) and finetune our method using few labeled instances of target domain. Modules in blue are inherited from DE-ViT while modules in orange are proposed by us. 
             New improvements include <b><font color=#2980B9>learnable instance features, instance reweighting, domain prompter, and finetuning</font></b>; More details about the modules please refer to our paper. 
             <br><br>
             We hightlight that all the modules are very <b><font color=#2980B9>lightwight</font></b> causing very negligible or even no cost. 
        </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Main Results</h2>
        <div class="content has-text-justified">
          A <b><font color=#2980B9>wide broader of existing detectors</font></b> are studied, incuding: 1) <b>typical FSOD</b> methods e.g., meta-RCNN, TFA, FSCE, and DeFRCN; 2) <b>CD-FSOD</b> methods e.g., distill-cdfsod; 3) <b>ViT-based</b> detector e.g., ViTDeT; 4) <b>open-set</b> detector e.g., DE-ViT and Detic. 
          <br><br>
          The comparsional results are as follows. ( "FT" means we apply the finetuning on the original model.)
          <p>
            <center>
              <img src="./imgs/result.png" width="95%"/>
            </center>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>






<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Citation</h2>
    <b>Please consider cite us if you find our tackled data, code, or model is useful to you.</b>      <br><br>
    Also feel free to ask questions or if you are interested in working on this topic together, thanks!   :) 
<br>
    <pre><code>
  @article{fu2024objectrelator,
      title={Objectrelator: Enabling cross-view object relation understanding in ego-centric and exo-centric videos},
      author={Fu, Yuqian and Wang, Runze and Bin, Ren and Guolei, Sun and Gong, Biao and Fu, Yanwei and Paudel, Danda Pani and Huang, Xuanjing and Van Gool, Luc},
      journal={ICCV2025},
      year={2025}
    }

  @article{fu2025cross,
      title={Cross-View Multi-Modal Segmentation@ Ego-Exo4D Challenges 2025},
      author={Fu, Yuqian and Wang, Runze and Fu, Yanwei and Paudel, Danda Pani and Van Gool, Luc},
      journal={arXiv preprint arXiv:2506.05856},
      year={2025}
    }
    </code></pre>
  </div>
</section>

</body>
</html>
