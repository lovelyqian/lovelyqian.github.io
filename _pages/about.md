---
permalink: /
title: "Welcome to Yuqian Fu's homepage"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Biography
------
I am currently a Ph.D. Candidate at <a href="https://fvl.fudan.edu.cn">FVL</a> Lab, Fudan University, advised by Prof. <a href="https://fvl.fudan.edu.cn/people/yugangjiang">Yu-Gang Jiang</a>, co-advised by Prof. <a href="https://yanweifu.github.io/">Yanwei Fu</a>, and Associate Professor <a href="http://vireo.cs.cityu.edu.hk/jingjing">Jingjing Chen</a>. Before that, I received my bachelorâ€™s degree in computer science from Zhejiang University of Technology, Hangzhou, China, in 2018, supervised by Associate Professor <a href ="http://www.homepage.zjut.edu.cn/congbai/">Cong Bai</a>.

My current research topics are computer vision and deep learning, and I am mainly focused on meta-learning, few-shot video action recognition, and cross-domain few-shot learning.


News
------
<ul>
  <li>[06/2022] Two papers are accepted by ACM MM 2022.</li>
  <li>[12/2021] I am recognized as an outstanding student of Fudan University.</li>
  <li>[06/2021] One paper is accepted by ACM MM 2021.</li>
  <li>[04/2021] One paper is accepted by ICMR 2021.</li>
  <li>[07/2020] One paper is accepted by ACM MM 2020.</li>
  <li>[12/2019] I am awarded the Chinese National Scholarship.</li>
  <li>[07/2019] One paper is accepted by ACM MM 2019.</li>
</ul>

<!-- Selected Publications
# To update this
------
<div class="img">
            <div id="pic" class="baguetteBox gallery">
                <img src="images/framework-MED2N.png" />
            </div>     
            <div class="details" >
            <p><a href="">ME-D2N: Multi-Expert Domain Decompositional Network for Cross-Domain Few-Shot Learning</a><br /><strong>Yuqian Fu</strong>, Yu Xie, Yanwei Fu, Jingjing Chen, Yu-Gang Jiang<br /> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2022.<br /> [<a href="">Paper Coming Soon</a>][<a href="https://github.com/lovelyqian/ME-D2N_for_CDFSL">Code</a>]</p>
            </div>

</div> -->

Publications
------
<table style="width:100%">
  <tr>
    <th>
      <img src="../images/framework-MED2N.png" width="350"/>
    </th>
    <th style="text-align:left">
            <span style="font-size:18px">ME-D2N: Multi-Expert Domain Decompositional Network for Cross-Domain Few-Shot Learning</span><br>
            <span style="font-size:16px">Yuqian Fu<span style="font-weight:normal">, Yu Xie, Yanwei Fu, Jingjing Chen, Yu-Gang Jiang</span></span><br>
            <span style="font-weight:normal;font-size:16px">ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2022.</span><br>
            <span style="font-weight:normal;font-size:16px">[<a href="">Paper Coming Soon</a>][<a href="https://github.com/lovelyqian/ME-D2N_for_CDFSL">Code</a>]</span>
    </th>
  </tr> 
</table>

<table style="width:100%">
  <tr>
    <th>
      <img src="../images/framework-MED2N.png" width="350"/>
    </th>
    <th style="text-align:left">
            <span style="font-size:18px">TGDM: Target Guided Dynamic Mixup for Cross-Domain Few-Shot Learning</span><br>
            <span style="font-size:16px">Linhai Zhuo, Yuqian Fu<span style="font-weight:normal">, Jingjing Chen, Yixin Cao, Yu-Gang Jiang</span></span><br>
            <span style="font-weight:normal;font-size:16px">ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2022.</span><br>
            <span style="font-weight:normal;font-size:16px">[<a href="">Paper Coming Soon</a>]</span>
    </th>
  </tr> 
</table>

<table style="width:100%">
  <tr>
    <th>
      <img src="../images/framework-MED2N.png" width="350"/>
    </th>
    <th style="text-align:left">
            <span style="font-size:18px">ME-D2N: Multi-Expert Domain Decompositional Network for Cross-Domain Few-Shot Learning</span><br>
            <span style="font-size:16px">Yuqian Fu<span style="font-weight:normal">, Yu Xie, Yanwei Fu, Jingjing Chen, Yu-Gang Jiang</span></span><br>
            <span style="font-weight:normal;font-size:16px">ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2022.</span><br>
            <span style="font-weight:normal;font-size:16px">[<a href="">Paper Coming Soon</a>][<a href="https://github.com/lovelyqian/ME-D2N_for_CDFSL">Code</a>]</span>
    </th>
  </tr> 

  <br>

  <tr>
    <th>
      <img src="../images/framework-TGDM.png" width="350"/>
    </th>
    <th style="text-align:left">
            <span style="font-size:18px">TGDM: Target Guided Dynamic Mixup for Cross-Domain Few-Shot Learning</span><br>
            <span style="font-size:16px">Linhai Zhuo, Yuqian Fu<span style="font-weight:normal">, Jingjing Chen, Yixin Cao, Yu-Gang Jiang</span></span><br>
            <span style="font-weight:normal;font-size:16px">ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2022.</span><br>
            <span style="font-weight:normal;font-size:16px">[<a href="">Paper Coming Soon</a>]</span>
    </th>
  </tr> 

  <br>

  <tr>
    <th>
      <img src="../images/framework-waveSAN.png" width="350"/>
    </th>
    <th style="text-align:left">
            <span style="font-size:18px">Wave-SAN: Wavelet based Style Augmentation Network for Cross-Domain Few-Shot Learning</span><br>
            <span style="font-size:16px">Yuqian Fu<span style="font-weight:normal">, Yu Xie, Yanwei Fu, Jingjing Chen, Yu-Gang Jiang</span></span><br>
            <span style="font-weight:normal;font-size:16px">arXiv preprint, 2022.</span><br>
            <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/pdf/2203.07656.pdf">Paper</a>]</span>
    </th>
  </tr> 

  <br>

   <tr>
    <th>
      <img src="../images/framework-metaFDMixup.png" width="350"/>
    </th>
    <th style="text-align:left">
            <span style="font-size:18px">Meta-FDMixup: Cross-Domain Few-Shot Learning Guided by Labeled Target Data</span><br>
            <span style="font-size:16px">Yuqian Fu<span style="font-weight:normal">, Yanwei Fu, Yu-Gang Jiang</span></span><br>
            <span style="font-weight:normal;font-size:16px">ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2021.</span><br>
            <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/pdf/2107.11978.pdf">Paper</a>][<a href="https://github.com/lovelyqian/Meta-FDMixup">Code</a>][<a href="https://www.youtube.com/watch?v=G8Mlde4FpsU">Youtube Video</a>][<a href="https://www.bilibili.com/video/BV1xT4y1f7B6?spm_id_from=333.999.0.0&vd_source=668a0bb77d7d7b855bde68ecea1232e7">Bilibili Video</a>]</span>
    </th>
  </tr> 

  <br>

   <tr>
    <th>
      <img src="../images/framework-ActionImitation.png" width="350"/>
    </th>
    <th style="text-align:left">
            <span style="font-size:18px">Can Action be Imitated? Learn to Reconstruct and Transfer Human Dynamics from Videos</span><br>
            <span style="font-size:16px">Yuqian Fu<span style="font-weight:normal">, Yanwei Fu, Yu-Gang Jiang</span></span><br>
            <span style="font-weight:normal;font-size:16px">International Conference on Multimedia Retrieval (<strong>ICMR</strong>). 2021. (<strong>Oral</strong>)</span><br>
            <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/pdf/2107.11756.pdf">Paper</a>][<a href="https://www.bilibili.com/video/BV1VY41147xt?spm_id_from=333.999.0.0">Bilibili Video</a>]</span>
    </th>
  </tr> 

  <br>
  
  <tr>
    <th>
      <img src="../images/framework-AMeFu.png" width="350"/>
    </th>
    <th style="text-align:left">
            <span style="font-size:18px">Depth Guided Adaptive Meta-Fusion Network for Few-shot Video Recognition</span><br>
            <span style="font-size:16px">Yuqian Fu<span style="font-weight:normal">, Li Zhang, Junke Wang, Yanwei Fu, Yu-Gang Jiang</span></span><br>
            <span style="font-weight:normal;font-size:16px">ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2020. (<strong>Oral</strong>)</span><br>
            <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/pdf/2010.09982.pdf">Paper</a>][<a href="https://github.com/lovelyqian/AMeFu-Net">Code</a>][<a href="https://www.youtube.com/watch?v=KqNYuZD5xdw">Youtube Video</a>][<a href="https://www.bilibili.com/video/BV1i44y1t78U?spm_id_from=333.999.0.0">Bilibili Video</a>]</span>
    </th>
  </tr> 

  <tr>
    <th>
      <img src="../images/framework-embodied.png" width="350"/>
    </th>
    <th style="text-align:left">
            <span style="font-size:18px">Embodied One-Shot Video Recognition: Learning from Actions of a Virtual Embodied Agent</span><br>
            <span style="font-size:16px">Yuqian Fu<span style="font-weight:normal">, Chengrong Wang, Yanwei Fu, Yu-Xiong Wang, Cong Bai, Xiangyang Xue, Yu-Gang Jiang</span></span><br>
            <span style="font-weight:normal;font-size:16px">ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2019. (<strong>Oral</strong>)</span><br>
            <span style="font-weight:normal;font-size:16px"> [<a href="http://www.cs.cmu.edu/~yuxiongw/research/Embodied_One-Shot_Video_Recognition_Learning_from_Actions_of_a_Virtual_Embodied_Agent.pdf">Paper</a>][<a href="https://github.com/lovelyqian/Embodied-One-Shot-Video-Recognition">Code</a>][<a href="http://www.sdspeople.fudan.edu.cn/fuyanwei/dataset/UnrealAction/">UnrealAction Dataset</a>]</span>
    </th>
  </tr> 
</table>


<!-- <ul>
  <li>
    <p><a href="">ME-D2N: Multi-Expert Domain Decompositional Network for Cross-Domain Few-Shot Learning</a><br /><strong>Yuqian Fu</strong>, Yu Xie, Yanwei Fu, Jingjing Chen, Yu-Gang Jiang<br /> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2022.<br /> [<a href="">Paper Coming Soon</a>][<a href="https://github.com/lovelyqian/ME-D2N_for_CDFSL">Code</a>]</p>
  </li>
  <li>
    <p><a href="">TGDM: Target Guided Dynamic Mixup for Cross-Domain Few-Shot Learning</a><br />Linhai Zhuo, <strong>Yuqian Fu</strong>, Jingjing Chen, Yixin Cao, Yu-Gang Jiang<br /> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2022.<br /> [<a href="">Paper Coming Soon</a>]</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2203.07656">Wave-SAN: Wavelet based Style Augmentation Network for Cross-Domain Few-Shot Learning</a><br /> <strong>Yuqian Fu</strong>, Yu Xie, Yanwei Fu, Jingjing Chen, Yu-Gang Jiang<br /> arXiv preprint, 2022. <br /> [<a href="https://arxiv.org/pdf/2203.07656.pdf">Paper</a>]</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.11978">Meta-FDMixup: Cross-Domain Few-Shot Learning Guided by Labeled Target Data</a><br /><strong>Yuqian Fu</strong>, Yanwei Fu, Yu-Gang Jiang<br /> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2021. <br /> [<a href="https://arxiv.org/pdf/2107.11978.pdf">Paper</a>][<a href="https://github.com/lovelyqian/Meta-FDMixup">Code</a>][<a href="https://www.youtube.com/watch?v=G8Mlde4FpsU">Youtube Video</a>][<a href="https://www.bilibili.com/video/BV1xT4y1f7B6?spm_id_from=333.999.0.0&vd_source=668a0bb77d7d7b855bde68ecea1232e7">Bilibili Video</a>]</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.11756">Can Action be Imitated? Learn to Reconstruct and Transfer Human Dynamics from Videos</a><br /><strong>Yuqian Fu</strong>, Yanwei Fu, Yu-Gang Jiang<br /> International Conference on Multimedia Retrieval (<strong>ICMR</strong>). 2021. (<strong>Oral</strong>)<br /> [<a href="https://arxiv.org/pdf/2107.11756.pdf">Paper</a>][<a href="https://www.bilibili.com/video/BV1VY41147xt?spm_id_from=333.999.0.0">Bilibili Video</a>]</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2010.09982">Depth Guided Adaptive Meta-Fusion Network for Few-shot Video Recognition
</a><br /><strong>Yuqian Fu</strong>, Li Zhang, Junke Wang, Yanwei Fu, Yu-Gang Jiang<br /> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2020. (<strong>Oral</strong>)<br /> [<a href="https://arxiv.org/pdf/2010.09982.pdf">Paper</a>][<a href="https://github.com/lovelyqian/AMeFu-Net">Code</a>][<a href="https://www.youtube.com/watch?v=KqNYuZD5xdw">Youtube Video</a>][<a href="https://www.bilibili.com/video/BV1i44y1t78U?spm_id_from=333.999.0.0">Bilibili Video</a>]</p>
  </li>
  <li>
    <p><a href="http://www.cs.cmu.edu/~yuxiongw/research/Embodied_One-Shot_Video_Recognition_Learning_from_Actions_of_a_Virtual_Embodied_Agent.pdf"> Embodied One-Shot Video Recognition: Learning from Actions of a Virtual Embodied Agent </a><br /> <strong>Yuqian Fu</strong>, Chengrong Wang, Yanwei Fu, Yu-Xiong Wang, Cong Bai, Xiangyang Xue, Yu-Gang Jiang<br /> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2019. (<strong>Oral</strong>) <br /> [<a href="http://www.cs.cmu.edu/~yuxiongw/research/Embodied_One-Shot_Video_Recognition_Learning_from_Actions_of_a_Virtual_Embodied_Agent.pdf">Paper</a>][<a href="https://github.com/lovelyqian/Embodied-One-Shot-Video-Recognition">Code</a>][<a href="http://www.sdspeople.fudan.edu.cn/fuyanwei/dataset/UnrealAction/">UnrealAction Dataset</a>]</p>
  </li>
</ul> -->







