---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
redirect_from:
  - /resume
---


{% include base_path %}


<ul>
 <li>
    <p><a href="">ObjectRelator: Enabling Cross-View Object Relation Understanding in Ego-Centric and Exo-Centric Videos</a><br/><strong>Yuqian Fu</strong>, Runze Wang, Bin Ren, Guolei Sun, Biao Gong, Yanwei Fu, Danda Pani Paudel, Xuanjing Huang, Luc Van Gool<br /> International Conference on Computer Vision  (<strong>ICCV</strong>), 2025. 
    <br /> [<a href="https://arxiv.org/pdf/2411.19083">Paper</a>][<a href="https://github.com/lovelyqian/ObjectRelator">Code</a>][<a href="">Youtube Video (Coming Soon) </a>][<a href="">Bilibili Video (Coming Soon) </a>][<a href="">Project Page (Coming Soon) </a>]</p>
  </li>
   <li>
    <p><a href="">XTrack: Multimodal Training Boosts RGB-X Video Object Trackers</a>
    <br/>Yuedong Tan, Zongwei Wu, <strong>Yuqian Fu</strong>, Zhuyun Zhou, Guolei Sun, Eduard Zamfir, Chao Ma, Danda Pani Paudel, Luc Van Gool, Radu Timofte<br /> International Conference on Computer Vision  (<strong>ICCV</strong>), 2025. 
    <br /> [<a href="">Paper</a>][<a href="">Code</a>]</p>
  </li>
     <li>
    <p><a href="">3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection</a>
    <br/>Yung-Hsu Yang, Luigi Piccinelli, Mattia Segu, Siyuan Li, Rui Huang, <strong>Yuqian Fu</strong>, Marc Pollefeys, Hermann Blum, Zuria Bauer<br /> International Conference on Computer Vision  (<strong>ICCV</strong>), 2025. 
    <br /> [<a href="">Paper</a>][<a href="">Code</a>]</p>
  </li>
    <li>
    <p><a href="">Understanding Museum Exhibits using Vision-Language Reasoning</a>
    <br/>Ada-Astrid Balauca, Sanjana Garai, Stefan Balauca, Rasesh Udayakumar Shetty, Naitik Agrawal, Dhwanil Subhashbhai Shah, <strong>Yuqian Fu</strong>, Xi Wang, Kristina Toutanova, Danda Pani Paudel, Luc Van Gool<br /> International Conference on Computer Vision  (<strong>ICCV</strong>), 2025. 
    <br /> [<a href="https://arxiv.org/pdf/2412.01370">Paper</a>][<a href="">Code</a>]</p>
  </li>
 <li>
    <p><a href="">RAG-6DPose: Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as Knowledge Base</a><br/>Kuanning Wang, <strong>Yuqian Fu</strong>, Tianyu Wang, Yanwei Fu, Longfei Liang, Yu-Gang Jiang, Xiangyang Xue<br /> International Conference on Intelligent Robots and Systems (<strong>IROS</strong>), 2025. 
    <br /> [<a href="https://arxiv.org/pdf/2506.18856">Paper</a>][<a href="https://sressers.github.io/RAG-6DPose/">Project Page</a>]</p>
  </li>
 <li>
    <p><a href="">Sequential Multi-Object Grasping with One Dexterous Hand</a><br />Sicheng He, Zeyu Shangguan, Kuanning Wang, Yongchong Gu,<strong>Yuqian Fu</strong>, Yanwei Fu, Daniel Seita<br /> International Conference on Intelligent Robots and Systems (<strong>IROS</strong>), 2025. 
    <br /> [<a href="https://arxiv.org/pdf/2503.09078">Paper</a>][<a href="https://hesic73.github.io/SeqMultiGrasp/">Project Page</a>]</p>
  </li>
 <li>
    <p><a href="">Cross-View Multi-Modal Segmentation @ Ego-Exo4D Challenges 2025</a><br /><strong>Yuqian Fu</strong>, Runze Wang, Yanwei Fu, Danda Pani Paudel, Luc Van Gool, and others<br /> <strong>2nd Place Award</strong> of Correspondences, Ego-Exo4D Challenge @ CVPRW, 2025. 
    <br /> [<a href="https://arxiv.org/abs/2506.05856">Paper</a>][<a href="https://github.com/lovelyqian/ObjectRelator">Code</a>]</p>
  </li>
    <li>
    <p><a href="">NTIRE2025 Challenge on Cross-Domain Few-Shot Object Detection: Methods and Results</a><br /><strong>Yuqian Fu</strong>, Xingyu Qiu, Bin Ren, Yanwei Fu, Radu Timofte, Nicu Sebe, Ming-Hsuan Yang, Luc Van Gool, and others<br />IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop (<strong>CVPRW</strong>), 2025. 
    <br /> [<a href="https://arxiv.org/pdf/2504.10685">Paper</a>][<a href="https://github.com/lovelyqian/NTIRE2025_CDFSOD">Code</a>]</p>
  </li>
    <li>
    <p><a href="">CAFuser: Condition-Aware Multimodal Fusion for Robust Semantic Perception of Driving Scenes</a><br />Tim Brödermann ‡, Christos Sakaridis, <strong>Yuqian Fu ‡</strong>, Luc Van Gool<br />IEEE Robotics and Automation Letters (<strong>RA-L</strong>), 2025. 
    <br /> [<a href="https://arxiv.org/pdf/2410.10791?">Paper</a>][<a href="https://github.com/timbroed/CAFuser">Code</a>][<a href="">Youtube Video (coming soon) </a>][<a href="">Bilibili Video (coming soon)</a>][<a href="">Project Page (combing soon)</a>]</p>
  </li>
    <li>
    <p><a href="">Locate Anything on Earth: Advancing Open-Vocabulary Object Detection for Remote Sensing Community</a><br />Jiancheng Pan, Yanxing Liu, <strong>Yuqian Fu ‡</strong>, Muyuan Ma, Jiahao Li, Danda Pani Paudel, Luc Van Gool, Xiaomeng Huang ‡<br /> The AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2025. 
    <br /> [<a href="https://arxiv.org/pdf/2408.09110">Paper</a>][<a href="https://github.com/jaychempan/LAE-DINO">Code</a>][<a href="">Youtube Video (coming soon) </a>][<a href="https://www.bilibili.com/video/BV1YWAkeMEGz/?vd_source=668a0bb77d7d7b855bde68ecea1232e7">Bilibili Video </a>][<a href="https://jianchengpan.space/LAE-website/index.html">Project Page</a>]</p>
  </li>
    <li>
    <p><a href="">Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector</a><br /><strong>Yuqian Fu</strong>, Yu Wang, Yixuan Pan, Lian Huai, Xingyu Qiu, Zeyu Shangguan, Tong Liu, Yanwei Fu, Luc Van Gool, Xingqun Jiang<br /> The European Conference on Computer Vision (<strong>ECCV</strong>), 2024. 
    <br /> [<a href="https://arxiv.org/pdf/2402.03094">Paper</a>][<a href="https://github.com/lovelyqian/CDFSOD-benchmark">Code</a>][<a href="https://www.youtube.com/watch?v=t5vREYQIup8">Youtube Video</a>][<a href="https://www.bilibili.com/video/BV17v4UetEdF/?vd_source=668a0bb77d7d7b855bde68ecea1232e7#reply113142138936707">Bilibili Video</a>][<a href="http://yuqianfu.com/CDFSOD-benchmark">Project Page</a>]</p>
  </li>
  <li>
    <p><a href="">Unified View Empirical Study for Large Pretrained Model on Cross-Domain Few-Shot Learning</a><br />Linhai Zhuo, <strong>Yuqian Fu</strong>, Jingjing Chen, Yixin Cao, Yu-Gang Jiang<br /> ACM Transactions on Multimedia Computing, Communications and Applications (<strong>TOMM</strong>), 2024. 
    <br /> [<a href="https://dl.acm.org/doi/full/10.1145/3673231?casa_token=ss6gdrAiQHkAAAAA:JLszbgQbVbh-Tp9DPPw4GCw0_0n6ZrcyAcUBvN5kxsubNSso3b31t4pupPhIjIcXxajpkq5_R9DoLQ">Paper</a>]</p>
  </li>
  <li>
    <p><a href="">MinD-3D: Reconstruct High-quality 3D objects in Human Brain</a><br />Jianxiong Gao, <strong>Yuqian Fu</strong>, Yun Wang, Xuelin Qian, Jianfeng Feng, Yanwei Fu <br/> The European Conference on Computer Vision (<strong>ECCV</strong>), 2024.<br /> [<a href="https://arxiv.org/abs/2312.07485">Paper</a>][<a href="https://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape">Code</a>][<a href="https://jianxgao.github.io/MinD-3D">Project Page</a>]</p>
  </li>
  <li>
    <p><a href="">Open-Vocabulary Video Relation Extraction</a><br />Wentao Tian, Zheng Wang, <strong>Yuqian Fu</strong>, Jingjing Chen, Lechao Cheng<br /> AAAI Conference on Artificial Intelligence  (<strong>AAAI</strong>), 2024.<br /> [<a href="https://arxiv.org/abs/2312.15670">Paper</a>]</p>
  </li>
  <li>
    <p><a href="">On the Importance of Spatial Relations for Few-shot Action Recognition</a><br />Yilun Zhang, <strong>Yuqian Fu</strong>, Xingjun Ma, Lizhe Qi, Jingjing Chen, Zuxuan Wu, Yu-Gang Jiang<br /> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2023.<br /> [<a href="https://arxiv.org/pdf/2308.07119.pdf">Paper</a>]</p>
  </li>
  <li>
    <p><a href="">StyleAdv: Meta Style Adversarial Training for Cross-Domain Few-Shot Learning</a><br /><strong>Yuqian Fu</strong>, Yu Xie, Yanwei Fu, Yu-Gang Jiang<br /> IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023. <br /> [<a href="https://arxiv.org/pdf/2302.09309.pdf">Paper</a>][<a href="https://github.com/lovelyqian/StyleAdv-CDFSL">Code</a>][<a href="https://youtu.be/YB-S2YF22mc">Youtube Video</a>][<a href="https://www.bilibili.com/video/BV1th4y1s78H/?spm_id_from=333.999.0.0&vd_source=668a0bb77d7d7b855bde68ecea1232e7">Bilibili Video</a>][<a href="http://yuqianfu.com/PJ-StyleAdv">Project Page</a>]</p>
  </li>
  <li>
    <p><a href="">Generalized Meta-FDMixup: Cross-Domain Few-Shot Learning Guided by Labeled Target Data</a><br /><strong>Yuqian Fu</strong>, Yanwei Fu, Jingjing Chen, Yu-Gang Jiang<br /> IEEE Transactions on Image Processing (<strong>TIP</strong>), 2022.<br /> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9942934">Paper</a>]</p>
  </li>
  <li>
    <p><a href="">ME-D2N: Multi-Expert Domain Decompositional Network for Cross-Domain Few-Shot Learning</a><br /><strong>Yuqian Fu</strong>, Yu Xie, Yanwei Fu, Jingjing Chen, Yu-Gang Jiang<br /> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2022.<br /> [<a href="https://arxiv.org/pdf/2210.05280.pdf">Paper</a>][<a href="https://github.com/lovelyqian/ME-D2N_for_CDFSL">Code</a>][<a href="https://www.youtube.com/watch?v=crCoaBLuFeA">Youtube Video</a>][<a href="https://www.bilibili.com/video/BV1GG4y1p7if/?vd_source=668a0bb77d7d7b855bde68ecea1232e7">Bilibili Video</a>]</p>
  </li>
  <li>
    <p><a href="">TGDM: Target Guided Dynamic Mixup for Cross-Domain Few-Shot Learning</a><br />Linhai Zhuo, <strong>Yuqian Fu</strong>, Jingjing Chen, Yixin Cao, Yu-Gang Jiang<br /> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2022.<br /> [<a href="https://arxiv.org/pdf/2210.05392.pdf">Paper</a>]</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2203.07656">Wave-SAN: Wavelet based Style Augmentation Network for Cross-Domain Few-Shot Learning</a><br /> <strong>Yuqian Fu</strong>, Yu Xie, Yanwei Fu, Jingjing Chen, Yu-Gang Jiang<br /> arXiv preprint, 2022. <br /> [<a href="https://arxiv.org/pdf/2203.07656.pdf">Paper</a>][<a href="https://github.com/lovelyqian/wave-SAN-CDFSL">Code</a>]</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.11978">Meta-FDMixup: Cross-Domain Few-Shot Learning Guided by Labeled Target Data</a><br /><strong>Yuqian Fu</strong>, Yanwei Fu, Yu-Gang Jiang<br /> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2021. <br /> [<a href="https://arxiv.org/pdf/2107.11978.pdf">Paper</a>][<a href="https://github.com/lovelyqian/Meta-FDMixup">Code</a>][<a href="https://www.youtube.com/watch?v=G8Mlde4FpsU">Youtube Video</a>][<a href="https://www.bilibili.com/video/BV1xT4y1f7B6?spm_id_from=333.999.0.0&vd_source=668a0bb77d7d7b855bde68ecea1232e7">Bilibili Video</a>][<a href="http://yuqianfu.com/PJ-MetaFDMixup">Project Page</a>]</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.11756">Can Action be Imitated? Learn to Reconstruct and Transfer Human Dynamics from Videos</a><br /><strong>Yuqian Fu</strong>, Yanwei Fu, Yu-Gang Jiang<br /> International Conference on Multimedia Retrieval (<strong>ICMR</strong>). 2021. (<strong>Oral</strong>)<br /> [<a href="https://arxiv.org/pdf/2107.11756.pdf">Paper</a>][<a href="https://www.bilibili.com/video/BV1VY41147xt?spm_id_from=333.999.0.0">Bilibili Video</a>]</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2010.09982">Depth Guided Adaptive Meta-Fusion Network for Few-shot Video Recognition
</a><br /><strong>Yuqian Fu</strong>, Li Zhang, Junke Wang, Yanwei Fu, Yu-Gang Jiang<br /> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2020. (<strong>Oral</strong>)<br /> [<a href="https://arxiv.org/pdf/2010.09982.pdf">Paper</a>][<a href="https://github.com/lovelyqian/AMeFu-Net">Code</a>][<a href="https://www.youtube.com/watch?v=KqNYuZD5xdw">Youtube Video</a>][<a href="https://www.bilibili.com/video/BV1i44y1t78U?spm_id_from=333.999.0.0">Bilibili Video</a>][<a href="http://yuqianfu.com/PJ-AMeFuNet">Project Page</a>]</p>
  </li>
  <li>
    <p><a href="http://www.cs.cmu.edu/~yuxiongw/research/Embodied_One-Shot_Video_Recognition_Learning_from_Actions_of_a_Virtual_Embodied_Agent.pdf"> Embodied One-Shot Video Recognition: Learning from Actions of a Virtual Embodied Agent </a><br /> <strong>Yuqian Fu</strong>, Chengrong Wang, Yanwei Fu, Yu-Xiong Wang, Cong Bai, Xiangyang Xue, Yu-Gang Jiang<br /> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2019. (<strong>Oral</strong>) <br /> [<a href="http://www.cs.cmu.edu/~yuxiongw/research/Embodied_One-Shot_Video_Recognition_Learning_from_Actions_of_a_Virtual_Embodied_Agent.pdf">Paper</a>][<a href="https://github.com/lovelyqian/Embodied-One-Shot-Video-Recognition">Code</a>][<a href="http://yuqianfu.com/UnrealAction-Dataset">UnrealAction Dataset</a>]</p>
  </li>
</ul>


If you find my works or codes help, please consider citing me. (●°u°●)​ 」




